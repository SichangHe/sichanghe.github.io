<!-- toc -->
# Artificial Intelligence

most supervised learning when ppl talk about

## ethics

- Mill's utilitarianism:
    maximize benefit for everyone, mostly equally

    bane:
    - conflict of interest
    - personal benefit
- Kant's formalism/ duty ethics:
    unconditional command for every individual

    bane: universal principle may harm specific people
- Locke's Right Ethics:
    individual has right simply by existence

    bane: what to do when two people's rights conflict
- Aristotle's virtue ethics:
    objective goodness from human qualities

    bane: how to find the "golden mean"

the golden rule (all above agrees with)

> Do unto others as you would have others do unto you

### codes of ethics

statements of general principles, followed by instructions for specific conduct

defiens duties the professional owes to
society/employers/clients/colleagues/subordinates/profession/self

### engineering design process

1. recognize problem/need, gather information
1. define problem/goal
1. generate/propose solution/method
1. evaluate benefit&cost of alternatives

### handling ethical issues

1. correct the problem
1. whistle blowing
1. resign in protest

## definition of artificial intelligence

- humanly → acting
- ration → math/theory

### Turing test

- NLP
- knowledge representation
- automated reasoning
- ML

#### total Turing test

perceptional abilities

- computer vision
- robotics

### thinking humanly

- get inside the working of human mind
- general problem solver
- cognitive science

### thinking rationally

- correctness
- logic
- fact-check

### knowledge-based system

- general-purpose search
- domain-specific knowledge
- knowledge bottleneck

## intelligent agent

### rational agent

1. prior knowledge of environment
1. performable action
1. performance measurement
1. perception

### task environment

PEAS: performance measurement, environment, actuators, sensors

properties

1. fully/partially observable
1. single/multiple agent
1. deterministic/stochastic
1. episodic/sequential
1. static/dynamic
1. known/unknown

### agent structure

- function: perception → action
- architecture: sensory → actuator

#### table-driven structure

$$
n_{entry}=\sum_{t=1}^T|P|^T
$$

- simplest
- e.g. industrial robot
- #cases explode

#### simple reflex agent

match input against rule, return action

- model-based: only work for fully observable
- goal-based
- utility-based: maximize gain expectation

## search

- breadth/depth first

## example application

### collaborative perception

- share raw data → huge overhead
- extract feature
- share object position

### anomaly detection

- bidirectional optimization

### uninformed search

- backtracking search

#### breadth-first search

- complete: will find shallowest goal if graph has finite depth
- optimal if path cost $g(n)$ is non-decreasing of depth $d$
- $O(b^d)$ where $b$ is branching factor

#### uniform-cost search

expand node with least path cost $g(n)$

- optimal
- $O(b^{1+\lfloor C^*/\epsilon\rfloor})$ where every action cost $≥\epsilon$

#### informed search (heuristic search)

#### A\* search (A-star search)

- cost $f(n)=g(n)+h(n)$
    - cost to reach node $g(n)$
    - estimated cost to goal, heuristic, $h(n)$
- optimal if $h(n)$ is admissible in tree search
    - admissibility: never overestimate
- optimal if $h(n)$ is consistent in graph search
    - consistency: triangle inequality $h(n)≤c(n,a,n')+h(n')$
- optimally efficient if $h(n)$ is consistent:
    expand fewest node among optimal algorithm

## reinforcement learning

- control system
- model-based vs model-free

### dynamic programming

- key: sub-problem

example

- Dijkstra's algorithm: per node
- Bellman-Ford algorithm: per hop

### discrete Markov decision process (discrete MDP)

finite tuple $\{S,A,\{P_{sa}\},\gamma,R\}$

- space $S$
- action $A$
- state transition probabilities $P_{sa}$
- discount factor $\gamma\in[0,1)$
- reward function $R$: evaluation metric
- total payoff. maximize this

    $$
    V=\sum_i \gamma^iR(s_i)
    $$

- policy $\pi:s\mapsto a$. find this

#### find optimal policy

optimal policy $\pi^*$

$$
\pi^*=\argmax_a \sum_{s'}P_{sa}(s')V^*(s')
$$

- mapping state to expected total payoff $V^\pi:s\mapsto\R$

    $$
    V^\pi(s)=\mathbb E\left[
        \sum_i\gamma^iR(s_i)\Bigr|s_0=s
    \right]=
    \mathbb E\left[
        [R(s)]+\gamma V^\pi(s')
    \right]\\[12pt] ⇒
    V^\pi(s)=R(s)+\gamma\sum_{s'}P_{s\pi(s)}(s')V^\pi(s')
    $$

    Bellman equation

##### value iteration

1. $V(s):=0$
1. Bellman update: $∀\ s,$

    $$
    V(s):=R(s)+\max_a\gamma\sum_{s'}P_{sa}(s')V(s')
    $$

- linear system
- Bellman back operator $V':=B(V)$
- sync/async update
- $\gamma$ force $V$ to converge $V^*$ exponentially

##### policy iteration

1. random $\pi$
1. repeat:

    $$
    V:=V^\pi\qquad\text{by Bellman equation}\\
    \pi(s):=\argmax_a\sum_{s'}P_{sa}(s')V(s')
    $$

- when converge, guarantee optimal policy
- high complexity: linear system very step

##### exploration and exploitation

- $\varepsilon$-greedy

    $$
    a=\begin{cases}
        \argmax V&probability\ 1-\varepsilon\\
        random\ a\in A&probability\ \varepsilon
    \end{cases}
    $$

    - $\varepsilon$ is small and decrease
- softmax

### continuous Markov decision process (continuous MDP)

$$
V(s)=R(s)+\gamma\max_a\mathbb E_{s'\sim P_{sa}}[V(s')]
$$

#### inverted pendulum

- kinematic model

#### discretization

- curse of dimensionality: $|S|=\R^n$
- bad for smooth function

4 ~ 8 dimension

#### value function approximation

approximate $V^*$

$$
V^*(s)=\theta^T\phi(s)
$$

trial

1. model/simulator: $s'\mapsto P_{sa}$
    - assume $A$ discrete

    $$
    s'=s+\Delta t\dot s
    $$

1. learn from data
    - $n$ trial, each with $T$ time step
    - supervised learning $(s_t,a_t)\mapsto s_{t+1}$
        - linear regression $s_{t+1}=As_t+Ba_t,A\in\R^{m × n},B\in\R^{n × d}$
        - deterministic/stochastic model
            - noise term $\varepsilon_t\in N(0,\varepsilon)$
        - model-based reinforcement learning
        - fitted value iteration

##### fitted value iteration

approximate $V^*(s)$ from $s^{(i)},i\in\{1,…,n\}$

1. trial: ramdomly sample $s^{(i)},i\in\{1,…,n\}$
1. initialization: $\theta:=0$
1. repeat for $i\in\{1,…,n\}$:
    1. repeat for $1\in A$:

        sample $s'_i\sim P_{sa}^{(i)},i\in\{1,…,k\}$

        $$
        q(a):=\frac{1}{k}\sum_{j=1}^k\left[
            R(s^{(i)})+\gamma V(s'_j)
        \right]
        $$

        <details><summary>is estimation of</summary>

        $$
        R(s)+\gamma\mathbb E_{s'\sim P_{sa}}[V(s')]
        $$

        </details>

    $$
    y^{(i)}:=\max_aq(a)
    $$

any regression model, e.g.

$$
\theta:=\argmin_\theta\sum_i \left(
    \theta^T\phi(s^{(i)})-y^{(i)}
\right)^2
$$

- for deterministic model, can set $k=1$

#### Mealy machine MDP

$R:S × A → \R$

Bellman equation:

$$
V^*(s)=\max_a \left[
    R(s,a)+\gamma\sum_{s'}P_{sa}(s')V^*(s')
\right]\\[12pt] ⇒
\pi^*=\argmax_a \left[
    R(s,a)+\gamma\sum_{s'}P_{sa}(s')V^*(s')
\right]
$$

#### finite horizon MDP

- finite tuple $\{S,A,\{P_{sa}\},T,R\}$
- time horizon $T\in(0,+∞)$
- maximize $\sum_{t=0}^TR(s_t,a_t)$
- action based on time $\pi_t^*$
- time-dependent dynamic

    $$
    V_t(s)=\mathbb E \left[
        \sum_{t'=t}^TR(s_{t'},a_{t'})
    \right]\\[12pt]
    V_t^*(s)=\begin{cases}
        \displaystyle\max_a \left[
            R^{(t)}(s,a)+\mathbb E_{s'\sim P_{sa}^{(t)}}(V_{t+1}^*(s'))
        \right]&t\in\{0,…,T-1\}\\
        \displaystyle\max_a \left[
            R^{(t)}(s,a)
        \right]&t=T
    \end{cases}
    $$

    - solution by dynamic programming: work way back from $V_T^*(s)$

##### linear quadratic regulation

- $S:\R^n,A:\R^d,n>d$
- linear transition with noise $P_{sa}$

    $$
    s_{t+1}=As_t+Ba_t+\omega_t
    $$

- negative quadratic reward to push system back\
    $u_t:\R^{u × n},v_t:\R^{n × n},\quad u_t,v_t≥0$

    $$
    R(s_t,a_t)=-s_t^Tu_ts_t-a_t^Tv_ta_t
    $$

#### policy searching method

- stochastic policy $\pi_\theta:S × A → \R$
- $\pi_\theta(s,a)$: probability of taking action $a$ at state $s$
- direct policy search: find reasonable $\theta$

    $$
    \max_\theta\mathbb E\left[
        \sum_{t=0}^T R(s_t,a_t)\Bigr|\pi_\theta
    \right]
    $$

    - fixed initial state
    - greedy stochastic gradient ascent
    - learning rate $\alpha\in(0,1]$

repeat:

1. sample $s_t,a_t$
1. reinforce

    $$
    \theta:=\theta+\alpha\sum_t\left[
        \frac{\nabla_\theta\pi_\theta(s_t,a_t)}{\pi_\theta(s_t,a_t)}
    \right]\sum_tR(s_t,a_t)
    $$

reason it converge: product rule

### partial observable MDP

#### reinforce with baseline

- arbitrary baseline $b(s)$
    - independent of $a$

$$
\theta:=\theta+\alpha\sum_t\left[
    \nabla_\theta\ln\pi_\theta(s_t,a_t)
\right](f(\tau)-b(s))
$$

### Monte Carlo method

- trial: $(s_t,a_t)$
- wait until end of episode return $G_t$
- $V(s_t):=V(s_t)+\alpha(G_t-V(s_t))$
- drawback: slow if episode long

### time-difference learning (TD-learning)

at time $t+1$, update $V(s_t)$

$$
V(s_t):=V(s_t)+\alpha(\delta_t)
$$

- TD error $\delta_t$

    $$
    \delta_t=R(s_{t+1})+\gamma V(s_{t+1})-V(s_t)
    $$

#### on-policy TD (SARSA)

- $\varepsilon$-greedy

1. initialize arbitrary reward $Q(s,a)$
1. repeat for episode
    1. initialize $S$
    1. behavior policy: select $a_t$ based on $Q$
    1. repeat for each step
        1. select potential $a'$ for $s'$ based on $Q$
        1. update $Q(s,a)$

            $$
            Q(s_t,a_t):=Q(s_t,a_t)+\alpha\left[
                R(s_t)+\gamma Q(s_{t+1},a')-Q(s_t,a_t)
            \right]
            $$

        1. $s:=s'$

        until $s$ is terminal

#### off-policy TD: Q-learning

same as SARSA except

$$
Q(s_t,a_t):=Q(s_t,a_t)+\alpha\left[
    R(s_t)+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)
\right]
$$

- greedy
